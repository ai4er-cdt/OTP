{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from gsw.conversions import CT_from_pt\n",
    "from gsw.density import sigma2\n",
    "from geopy.distance import distance\n",
    "from scipy.integrate import simpson\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_depth(v: np.ndarray, \n",
    "              x_Z: np.ndarray, \n",
    "              x_lon: np.ndarray, \n",
    "              d: np.ndarray, \n",
    "              lat: int) -> np.ndarray:\n",
    "    _, n_lons, _, n_times = v.shape\n",
    "    ix = list(x_Z).index(d[lat])+1\n",
    "    moc = np.empty(n_times)\n",
    "    for t in range(n_times):\n",
    "        # inner integral over depth\n",
    "        inner = simpson(y=np.nan_to_num(v[lat, :, :ix, t]), x=x_Z[:ix], axis=1)\n",
    "        # outer integral over longitude\n",
    "        moc[t] = -simpson(y=inner, x=x_lon[lat, :]) / 1e6\n",
    "    return moc\n",
    "\n",
    "def lat_density(v: np.ndarray, \n",
    "                x_Z: np.ndarray, \n",
    "                x_lon: np.ndarray, \n",
    "                lighter: np.ndarray,\n",
    "                empty: np.ndarray,\n",
    "                isopycnals: np.ndarray,\n",
    "                lat: int) -> np.ndarray:\n",
    "    _, n_lons, _, n_times = v.shape\n",
    "    moc = np.empty(n_times)\n",
    "\n",
    "    for t in range(n_times):\n",
    "        for l in range(n_lons):\n",
    "            if not (lighter[lat, l, t] or empty[lat, l, t]):\n",
    "                ix = isopycnals[lat, l, t]\n",
    "            else:\n",
    "                ix = 0\n",
    "            v[lat, l, ix:, t] = 0.\n",
    "\n",
    "        # inner integral over depth\n",
    "        inner = simpson(y=v[lat, :, :, t], x=x_Z, axis=1)\n",
    "        # outer integral over longitude\n",
    "        moc[t] = -simpson(y=inner, x=x_lon[lat, :]) / 1e6\n",
    "    return moc\n",
    "\n",
    "def psi(d: float|np.ndarray, \n",
    "        v: np.ndarray, \n",
    "        x_lon: np.ndarray, \n",
    "        x_Z: np.ndarray,\n",
    "        s2: Optional[np.ndarray],\n",
    "        use_density: bool=False) -> np.ndarray:\n",
    "    n_lats, _, n_depths, _ = v.shape\n",
    "    d = np.array([d]*n_lats) if np.isscalar(d) else np.asarray(d)\n",
    "    assert len(d) == n_lats\n",
    "    if use_density:\n",
    "        # find water columns lighter than d\n",
    "        lighter = np.less_equal(s2[:, :, 0, :], d[:, np.newaxis, np.newaxis])\n",
    "        # find missing water columns\n",
    "        empty = np.isnan(s2).all(axis=2)\n",
    "        # find isopycnals (defaults to 0)\n",
    "        isopycnals = np.argmax(np.less_equal(s2, d[:, np.newaxis, np.newaxis, np.newaxis]), axis=2)\n",
    "        # any defaults (0 values) represent entire water columns which are heavier than d\n",
    "        isopycnals[isopycnals == 0] = n_depths\n",
    "        f = lat_density\n",
    "        _args = [v.copy(), x_Z, x_lon, lighter, empty, isopycnals]\n",
    "    else:\n",
    "        f = lat_depth\n",
    "        _args = [v, x_Z, x_lon, d]\n",
    "\n",
    "    # parallelise over latitudes\n",
    "    # NOTE: in my testing, I found that parallelising over latitudes was more efficient than any other dimension\n",
    "    with Pool(cpu_count()) as pool:\n",
    "            results = pool.starmap(f, [(*_args, lat) for lat in range(n_lats)]) \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bolus = True\n",
    "density_precision = 100\n",
    "data_path = \"/mnt/g/My Drive/GTC/ecco_data\"\n",
    "\n",
    "sections = [\"26N\", \"30S\", \"55S\", \"60S\"]\n",
    "coordinates = [\"latitude\", \"longitude\", \"Z\", \"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching monthly mean velocities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching bolus velocities\n",
      "using density: fetching temperature and salinity for calculation\n",
      "calculating streamfunction for all densities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:47<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching monthly mean velocities\n",
      "fetching bolus velocities\n",
      "calculating streamfunction for all depths\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:42<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for use_density in [True, False]:\n",
    "    for section in sections:\n",
    "        print(\"fetching monthly mean velocities\")\n",
    "        vm = xr.open_mfdataset(f\"{data_path}_full/{section}/ECCO_L4_OCEAN_VEL_05DEG_MONTHLY_V4R4/*.nc\",\n",
    "                            coords=\"minimal\",\n",
    "                            data_vars=\"minimal\",\n",
    "                            parallel=True, compat=\"override\")\n",
    "        vm = vm[[\"NVEL\"]].transpose(*coordinates).isel(Z=slice(None, None, -1)).fillna(0.)\n",
    "        vm = vm.rename({\"NVEL\": \"vm\"})\n",
    "        if use_bolus:\n",
    "            print(\"fetching bolus velocities\")\n",
    "            ve = xr.open_mfdataset(f\"{data_path}_full/{section}/ECCO_L4_BOLUS_05DEG_MONTHLY_V4R4/*.nc\",\n",
    "                                coords=\"minimal\",\n",
    "                                data_vars=\"minimal\",\n",
    "                                parallel=True, compat=\"override\")\n",
    "            ve = ve[[\"NVELSTAR\"]].transpose(*coordinates).isel(Z=slice(None, None, -1)).fillna(0.)\n",
    "            ve = ve.rename({\"NVELSTAR\": \"ve\"})\n",
    "        if use_density:\n",
    "            print(\"using density: fetching temperature and salinity for calculation\")\n",
    "            density = xr.open_mfdataset(f\"{data_path}_full/{section}/ECCO_L4_TEMP_SALINITY_05DEG_MONTHLY_V4R4/*.nc\",\n",
    "                                        coords=\"minimal\",\n",
    "                                        data_vars=\"minimal\",\n",
    "                                        parallel=True, compat=\"override\")\n",
    "            density = density[[\"THETA\", \"SALT\"]].transpose(*coordinates).isel(Z=slice(None, None, -1))\n",
    "            ct = CT_from_pt(SA=density[\"SALT\"], pt=density[\"THETA\"])\n",
    "            s2 = sigma2(SA=density[\"SALT\"], CT=ct).to_dataset()\n",
    "            s2 = s2.rename({list(s2.data_vars)[0]: \"sigma_2\"})\n",
    "        v = vm[\"vm\"] + ve[\"ve\"] if use_bolus else vm[\"vm\"]\n",
    "\n",
    "        if section == \"26N\":\n",
    "            slices = [(-97, -82), (-81, -14)]\n",
    "            condition = False\n",
    "            for (x, y) in slices:\n",
    "                condition = condition | ((v.longitude >= x) & (v.longitude <= y))\n",
    "            v = v.where(condition, drop=True)\n",
    "            if use_density: s2 = s2.where(condition, drop=True)\n",
    "\n",
    "        grid = np.array([[(lat, lon) for lon in v[\"longitude\"].to_numpy()] for lat in v[\"latitude\"].to_numpy()])\n",
    "        # get x-coordinates of longitude measurements\n",
    "        x_lon = np.array([[0.]+[distance(latitude[i+1], latitude[i]).meters\n",
    "                                for i in range(grid.shape[1]-1)] \n",
    "                                for latitude in grid])\n",
    "        # rounding to cm\n",
    "        x_lon = np.round(np.cumsum(x_lon, -1), 2)\n",
    "        # get x-coordinates of depth measurements\n",
    "        x_Z = v[\"Z\"].to_numpy()\n",
    "        # unsure if -ve is a problem but getting rid of them just in case\n",
    "        x_Z += max(abs(x_Z))\n",
    "\n",
    "        v_np = v.to_numpy()\n",
    "        if use_density: s2_np = s2[\"sigma_2\"].to_numpy()\n",
    "\n",
    "        if use_density:\n",
    "            temp = s2_np[~np.isnan(s2_np)]\n",
    "            sf_range = np.linspace(min(temp)-0.1, max(temp)+0.1, density_precision)\n",
    "            outfile = open(f\"single_lats/{section}_density_range.pickle\", \"wb\")\n",
    "            pickle.dump(sf_range, outfile); outfile.close()\n",
    "        else:\n",
    "            s2_np = None\n",
    "            sf_range = x_Z\n",
    "\n",
    "        args = [v_np, x_lon, x_Z, s2_np, use_density]\n",
    "        # calculate the streamfunction at all possible densities/depths\n",
    "        if use_density: print(\"calculating streamfunction for all densities\")\n",
    "        else: print(\"calculating streamfunction for all depths\")\n",
    "        streamfunction = np.array([psi(d, *args) for d in tqdm(sf_range)])\n",
    "        outpath = f\"single_lats/{section}_sf\"\n",
    "        outpath = outpath + \"_density.pickle\" if use_density else outpath + \"_depth.pickle\"\n",
    "        outfile = open(outpath, \"wb\")\n",
    "        pickle.dump(streamfunction, outfile); outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
