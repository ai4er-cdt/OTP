{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Trainings\n",
    "\n",
    "_Author: Aline Van Driessche_\n",
    "\n",
    "This notebook contains all code needed to perform Neural Network trainings (1D and 2D) on individual latitude information available from ECCO. Read section headings for more details on setup, preprocessing, and model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "Importing relevant modules and custom scripts. A number of helper functions are also defined to clean up experiment code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import paths\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from models.utils import *\n",
    "from models.plotting_utils import *\n",
    "from sklearn.metrics import *\n",
    "from models import train, CNN1D, CNN2D\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A helper function to subset 30S, since the satellite observable variables do not always exist for all longitudes - the values are taken from an earlier defined basin function\n",
    "def mask_30S(inputs):\n",
    "    mask = np.logical_or(np.logical_or(np.logical_and(inputs.longitude >= -180, \n",
    "                                                      inputs.longitude <= -71),\n",
    "                                       np.logical_and(inputs.longitude >= 31, \n",
    "                                                      inputs.longitude <= 115)),\n",
    "                         np.logical_and(inputs.longitude >= 153, \n",
    "                                        inputs.longitude <= 180))\n",
    "    masked = inputs.where(mask, drop = True)\n",
    "\n",
    "    return masked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to align the in-and outputs of two datasets with the inputs\n",
    "def align_dates(inputs, outputs):\n",
    "\n",
    "    # Align the dates according to the data available for in- and output\n",
    "    date_range = (inputs.time.values[0], inputs.time.values[-1]) # grabbing start and end date for inputs\n",
    "    date_range = tuple([str(d).split('T')[0] for d in date_range]) # extracting just the date, w/o time\n",
    "    print('Date range to align to:', date_range)\n",
    "    return align_inputs_outputs(inputs, outputs, date_range = date_range, ecco=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A helper function to format the input files based on a given latitude\n",
    "def format_lat_lon(value):\n",
    "    \"\"\"Format latitude or longitude with N/S or E/W suffix.\"\"\"\n",
    "    if value < 0:\n",
    "        return f\"{abs(value)}S\"\n",
    "    else:\n",
    "        return f\"{value}N\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Variables\n",
    "\n",
    "All variables that might be worth changing during experiments. Of particular interest is `no_zonal_averages` and `zonal_avgs` which controls which variables are averaged zonally and which variables are left with all longitudinal information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_home = paths.LOCAL_DIR\n",
    "lats = [26, -30, -55, -60]\n",
    "\n",
    "lat = lats[2]\n",
    "\n",
    "zonal_avg = ['time']\n",
    "no_zonal_avgs = ['time', 'longitude']   #for 2D training this setting is necessary\n",
    "\n",
    "remove_season = False\n",
    "remove_trend = False\n",
    "\n",
    "input_vars = ['OBP', 'ZWS'] #other options are 'SSH', 'SSS', 'SST'\n",
    "\n",
    "timelag=1\n",
    "\n",
    "inputs_fp = f\"{data_home}/ecco_data_minimal/{format_lat_lon(lat)}.nc\"\n",
    "outputs_fp = f\"{data_home}/ecco_data_minimal/{format_lat_lon(lat)}_moc_density.pickle\"\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "dropout = 0.1\n",
    "n_pure_layers = 1\n",
    "n_mix_layers = 1\n",
    "n_channels_mult = 4\n",
    "kernel_size = 3           #For 2D input change this to (3x3)\n",
    "model_iterations = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and processing relevant data\n",
    "Data loading function that extracted and preprocesses the data as defined in the “Experiment Variables” section and returns the in- and output for training.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_data(lat=lat, inputs_fp=inputs_fp, outputs_fp=outputs_fp, timelag=timelag, coords = zonal_avg, input_vars=input_vars, remove_trend=remove_trend, remove_season=remove_season):\n",
    "\n",
    "    # Retrieve the input covariates (ecco surface variables)\n",
    "    inputs = xr.open_dataset(inputs_fp).isel(latitude=slice(1,2))\n",
    "\n",
    "    # Some latitudes need specific preprocessing because of missing data or land that will obstruct training\n",
    "    if lat == -30:\n",
    "        inputs = mask_30S(inputs) \n",
    "    if lat == -55:\n",
    "        inputs = inputs.dropna('longitude', how = 'all')\n",
    "        \n",
    "    # Retrieve the output streamfunctions to train on (in density space), this file contains the latitude right above and right below so we take the middle one\n",
    "    with open(outputs_fp, 'rb') as f:\n",
    "        outputs = pickle.load(f).astype(np.float64)\n",
    "    outputs = np.expand_dims(outputs, 1) # grabbing just the middle latitude of interest \n",
    "\n",
    "    # Convert the data to an xarray dataset structure similar to the input data\n",
    "    outputs = xr.Dataset(\n",
    "                data_vars = {'moc' : ([\"time\", \"latitude\"], outputs)}, \n",
    "                coords = {'time' : inputs.time, 'latitude' : np.atleast_1d(lat),}\n",
    "            )\n",
    "    \n",
    "    # Align the date ranges\n",
    "    inputs, outputs = align_inputs_outputs(inputs, outputs, ecco=False)\n",
    "    \n",
    "    # Input data preprocessing (standardize)\n",
    "    pp_data_surface = apply_preprocessing(inputs,\n",
    "                              mode=\"inputs\",\n",
    "                              remove_season=remove_season,\n",
    "                              remove_trend=remove_trend,\n",
    "                              standardize=True,\n",
    "                              lowpass=False)\n",
    "    X = reshape_inputs(pp_data_surface, history=timelag, keep_coords=coords, data_vars=input_vars)\n",
    "    \n",
    "    # Necessary transpose for convolutions with PyTorch Tensors + adjusting the shape to suit as CNN input\n",
    "    dims = list(range(X.ndim))  \n",
    "    dims[1], dims[-1] = dims[-1], dims[1]\n",
    "    X = np.transpose(X, dims)\n",
    "    if len(X.shape) == 2:\n",
    "        X = np.expand_dims(X, axis=-1)\n",
    "    \n",
    "    # MOC strength preprocessing (don't standardize)\n",
    "    strength = apply_preprocessing(outputs,\n",
    "                               mode=\"outputs\",\n",
    "                               remove_season=remove_season,\n",
    "                               remove_trend=remove_trend,\n",
    "                               standardize=False,\n",
    "                               lowpass=False)\n",
    "    strength_np = strength.moc.squeeze().values\n",
    "    \n",
    "    # If the history parameter changed the input shape, change the outputs accordingly\n",
    "    y = strength_np[-X.shape[0]:]\n",
    "    \n",
    "    \"\"\"\n",
    "    print()\n",
    "    print('Inputs shape:', X.shape)\n",
    "    print('Extra inputs shape:', X_extra.shape)\n",
    "    print('Outputs shape:', y.shape)\n",
    "    \"\"\"\n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, y = retrieve_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running Experiments\n",
    "\n",
    "Data is extracted and preprocessed as defined in the \"Experiment Variables\" section. Grid search with cross-validation is performed to find the best regularization weights for ElasticNet (`alpha` and `L1_wt`); see the [`statsmodels` documentation](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html#statsmodels.regression.linear_model.OLS.fit_regularized) for more details. Our five metrics (RMSE, MAE, MAPE, $R^2$, correlation) are then calculated and saved to a text file, and two helpul plots are saved as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'models.train' from 'C:\\\\Users\\\\aline\\\\OTP\\\\models\\\\train.py'>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T18:27:28.135365300Z",
     "start_time": "2024-03-15T18:27:28.092569500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def train_1DCNN(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, stratify=None, random_state=123456)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, shuffle=False, stratify=None, random_state=123456)\n",
    "\n",
    "    device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    Xt_train = t.tensor(X_train, device=device)\n",
    "    Yt_train = t.tensor(y_train, device=device)\n",
    "    Xt_valid = t.tensor(X_valid, device=device).double()\n",
    "    Yt_valid = t.tensor(y_valid, device=device).double()\n",
    "    Xt_test = t.tensor(X_test, device=device).double()\n",
    "    Yt_test = t.tensor(y_test, device=device).double()\n",
    "\n",
    "    model = CNN1D.CNN1D(\n",
    "        n_pure_layers=n_pure_layers, \n",
    "        n_mix_layers=n_mix_layers, \n",
    "        n_features=Xt_train.shape[1], \n",
    "        n_channels=Xt_train.shape[1]*n_channels_mult,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout, \n",
    "    ).double()\n",
    "\n",
    "    model= train.train_model( #, val_loss\n",
    "        model=model, \n",
    "        X_train = Xt_train, \n",
    "        y_train = Yt_train, \n",
    "        X_val = Xt_valid,\n",
    "        y_val = Yt_valid, \n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    # Extracting all predictions in time order\n",
    "    train_set_pred = train.predict(model, Xt_train, Yt_train)\n",
    "    valid_set_pred = train.predict(model, Xt_valid, Yt_valid)\n",
    "    test_set_pred = train.predict(model, Xt_test, Yt_test)\n",
    "    \n",
    "    predictions = {\n",
    "        \"train_set_pred\": train_set_pred, \n",
    "        \"valid_set_pred\": valid_set_pred, \n",
    "        \"test_set_pred\": test_set_pred, \n",
    "        \"all_preds\": np.concatenate([train_set_pred, valid_set_pred, test_set_pred])\n",
    "    }\n",
    "    \n",
    "    print('test set', np.array(test_set_pred).shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    metrics = {\n",
    "        \"rmse\": round(root_mean_squared_error(test_set_pred, y_test), 3),\n",
    "        \"mae\": round(mean_absolute_error(test_set_pred, y_test), 3),\n",
    "        \"mape\": round(mean_absolute_percentage_error(test_set_pred, y_test), 3)*100,        \n",
    "        \"cmape\": round(custom_MAPE(np.array(test_set_pred).squeeze(), y_test.detach().cpu().numpy(), threshold=0.5), 3)*100,\n",
    "        \"test_corr\": round(np.corrcoef(np.array(test_set_pred).squeeze(), y_test)[0, 1], 3),\n",
    "        }\n",
    "    \n",
    "    return predictions, metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T18:27:58.423396700Z",
     "start_time": "2024-03-15T18:27:58.393100800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axes: ['time', 'history', 'feature']\n",
      "variables: ['OBP', 'ZWS']\n",
      "shape: (288, 1, 2)\n",
      "device: cpu\n",
      "137 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 681/5000 [00:02<00:15, 272.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'models.train' has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m X, y \u001B[38;5;241m=\u001B[39m retrieve_data()\n\u001B[1;32m----> 2\u001B[0m train_1DCNN(X, y)\n",
      "Cell \u001B[1;32mIn[57], line 34\u001B[0m, in \u001B[0;36mtrain_1DCNN\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m     24\u001B[0m model\u001B[38;5;241m=\u001B[39m train\u001B[38;5;241m.\u001B[39mtrain_model( \u001B[38;5;66;03m#, val_loss\u001B[39;00m\n\u001B[0;32m     25\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel, \n\u001B[0;32m     26\u001B[0m     X_train \u001B[38;5;241m=\u001B[39m Xt_train, \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     30\u001B[0m     early_stopping\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     31\u001B[0m )\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Extracting all predictions in time order\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m train_set_pred \u001B[38;5;241m=\u001B[39m train\u001B[38;5;241m.\u001B[39mpredict(model, Xt_train, Yt_train)\n\u001B[0;32m     35\u001B[0m valid_set_pred \u001B[38;5;241m=\u001B[39m train\u001B[38;5;241m.\u001B[39mpredict(model, Xt_valid, Yt_valid)\n\u001B[0;32m     36\u001B[0m test_set_pred \u001B[38;5;241m=\u001B[39m train\u001B[38;5;241m.\u001B[39mpredict(model, Xt_test, Yt_test)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'models.train' has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "X, y = retrieve_data()\n",
    "train_1DCNN(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T18:28:02.243906700Z",
     "start_time": "2024-03-15T18:27:59.487092700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To loop over several models and compare the performances\n",
    "models = []\n",
    "history = 6 #np.arange(6)\n",
    "for i in range(10):\n",
    "    X, y= get_input(outputs, inputs, history=history, coords=[\"time\", \"longitude\"])\n",
    "    Xt_train, Yt_train, Xt_valid, Yt_valid, Xt_test, Yt_test = define_train_and_validation_sets(X, y)\n",
    "    models.append(train_CNN(Xt_train, Yt_train))\n",
    "\n",
    "predictions_models = []\n",
    "metrics_models = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    X, y= get_input(outputs, inputs, history=history)\n",
    "    Xt_train, Yt_train, Xt_valid, Yt_valid, Xt_test, Yt_test = define_train_and_validation_sets(X, y)\n",
    "    predictions, metrics = predict_CNN(models[i], Xt_train, Yt_train, Xt_valid, Yt_valid, Xt_test, Yt_test )\n",
    "    predictions_models.append(predictions)\n",
    "    metrics_models.append(metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_models)\n",
    "summary_metrics_df = pd.DataFrame({\n",
    "                \"mean\": metrics_df.mean().round(3), \n",
    "                \"std\": metrics_df.std().round(3)}\n",
    "            )\n",
    "display(summary_metrics_df)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_models)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_df.to_csv(f\"{paths.LOCAL_DIR}/{lat}_history.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'H:/.shortcut-targets-by-id/1wvJjD0RMTujKYaXQapEiGk-Mx03_KSin/GTC/-30_history.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m plot_30 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpaths\u001B[38;5;241m.\u001B[39mLOCAL_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/-30_history.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m plot_55 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpaths\u001B[38;5;241m.\u001B[39mLOCAL_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/-55_history.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m plot_60 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpaths\u001B[38;5;241m.\u001B[39mLOCAL_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/-60_history.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\mambaforge\\envs\\gtc\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1011\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1012\u001B[0m     dialect,\n\u001B[0;32m   1013\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1020\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1021\u001B[0m )\n\u001B[0;32m   1022\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1024\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[1;32m~\\mambaforge\\envs\\gtc\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    615\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    617\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 618\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    620\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\mambaforge\\envs\\gtc\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1615\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1617\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1618\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[1;32m~\\mambaforge\\envs\\gtc\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1876\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1877\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1878\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[0;32m   1879\u001B[0m     f,\n\u001B[0;32m   1880\u001B[0m     mode,\n\u001B[0;32m   1881\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1882\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1883\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   1884\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[0;32m   1885\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1886\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1887\u001B[0m )\n\u001B[0;32m   1888\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1889\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\mambaforge\\envs\\gtc\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    874\u001B[0m             handle,\n\u001B[0;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m    876\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[0;32m    877\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    879\u001B[0m         )\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'H:/.shortcut-targets-by-id/1wvJjD0RMTujKYaXQapEiGk-Mx03_KSin/GTC/-30_history.csv'"
     ]
    }
   ],
   "source": [
    "plot_30 = pd.read_csv(f\"{paths.LOCAL_DIR}/-30_history.csv\")\n",
    "plot_55 = pd.read_csv(f\"{paths.LOCAL_DIR}/-55_history.csv\")\n",
    "plot_60 = pd.read_csv(f\"{paths.LOCAL_DIR}/-60_history.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T18:24:49.687502500Z",
     "start_time": "2024-03-15T18:24:49.193127500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 3.5))  # Optional: specifies the figure size\n",
    "plt.plot(plot_30.index, plot_30['mape'], marker='x', linestyle='-', linewidth=1.5, label='30S indo-pacific')  \n",
    "plt.plot(plot_55.index, plot_55['mape'], marker='x', linestyle='-', linewidth=1.5, label='55S southern') \n",
    "plt.plot(plot_60.index, plot_60['mape'], marker='x', linestyle='-', linewidth=1.5, label='60S southern') \n",
    "plt.xlabel('amount of historical data included in training [months]', fontsize=12)\n",
    "plt.ylabel('MAPE [%]', fontsize=12, weight=\"bold\")\n",
    "plt.legend(fontsize=12) \n",
    "plt.tick_params(labelsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjusts subplot params so that the subplot(s) fits in to the figure area.\n",
    "plt.savefig(f\"C:/Users/aline/OTP/plots/history1DCNN.png\", dpi=400)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-15T18:24:49.687502500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#predictions_df = pd.DataFrame(predictions)\n",
    "#mean_y = {col: np.mean(np.stack(predictions_df[col].values), axis=0) for col in predictions_df}\n",
    "\n",
    "predictions = predictions_models[9]\n",
    "\n",
    "print(len(predictions['all_preds']))\n",
    "\n",
    "time = outputs.time.values[-X.shape[0]:]\n",
    "fig, ax = timeseries_comparison(predictions['all_preds'], \n",
    "                                y,\n",
    "                                time, \n",
    "                                len(Xt_train))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, squeeze=True, figsize=(10, 6))\n",
    "\n",
    "all_moc = [y, y]\n",
    "all_pred = [predictions_models[0]['all_preds'], predictions_models[1]['all_preds']]\n",
    "sections = ['without historical data: RMSE = 3.97, MAPE$_{0.5}\\\\downarrow$ = 61.30, correlation = 0.85.', 'with historical data (6m): RMSE = 4.95, MAPE$_{0.5}\\\\downarrow$ = 42.90, correlation = 0.56.']\n",
    "\n",
    "for index, ax in enumerate(axs):\n",
    "    ax.plot(all_moc[index], label=\"ECCO\")\n",
    "    ax.plot(all_pred[index], label=\"Predicted\")\n",
    "    # all this stuff is emilio's plotting code\n",
    "    y_lower, y_upper = ax.get_ylim()\n",
    "    x_pos = np.arange(len(Xt_train))\n",
    "    ax.fill_between(x = x_pos, \n",
    "                    y1 = np.repeat(y_lower, len(x_pos)), \n",
    "                    y2 = np.repeat(y_upper, len(x_pos)),\n",
    "                    alpha = 0.2, \n",
    "                    color = 'gray')\n",
    "    ax.margins(x = 0, y = 0)\n",
    "    ax.set_title(sections[index])\n",
    "# and here also, just emilio's code\n",
    "ax.set_xticks(np.arange(0, 324, 60), np.arange(1992, 2019, 5), rotation=45)\n",
    "ax.legend(loc = 'lower right', edgecolor = 'black', framealpha = 1)\n",
    "fig.supylabel(\"MOC Strength [Sv]\", weight=\"bold\")\n",
    "plt.tight_layout()\n",
    "# dpi makes the plot more hd\n",
    "plt.savefig(f\"C:/Users/aline/OTP/plots/55S_historicaldata.png\", dpi=400)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Alternate view: looking at a scatterplot of predicted vs. actual\n",
    "y_pred_train = np.array(mean_y['train_set_pred']).squeeze()\n",
    "y_pred_test = np.array(mean_y['test_set_pred']).squeeze()\n",
    "\n",
    "fig, ax = pred_vs_actual(y_pred_train, y_pred_test, y_train, y_test)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
